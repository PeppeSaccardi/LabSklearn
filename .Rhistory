## Classifier and implemented methods
For almost every classification problem, and more generally for all well-posed
problem that can be solved using machine learning algorithm, we have to train the classifier on a subset of the original dataset, namely the training set, and then evaluate it in
according to a performance measure on a separate subset of the dataset: test
set.
We used the Soft Margin Classifier since although it could produce mis-classified
examples, it has some good generalization properties.
As we saw during the course, it is a maximization problem regarding the margin in
the training examples with a regularization terms that can be formalized as a
minimization problem where
<img src="eq.jpg" alt="drowing" width="200"/>
## On the research of the best hyper-parameters $C$ and $\gamma$
Once the classifier has been chosen, it is possible and recommended to search the hyper-parameter space for the best cross validation score.
For our purposes we used the **GridSearchCV** that exhaustively considers all parameter combinations given as input, and performs for all possible combinations a k-folds cross validation using the accuracy as evaluation measure.
```python
import sklearn
from sklearn.svm import SVC
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import learning_curve
